{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définitions des métriques\n",
    "Un cas est positif lorsque l'instance est dans la classe qu'on teste. \n",
    "\n",
    "#### Précision\n",
    "Pour les problèmes de classification binaire, la précision est le pourcentage des cas positifs prédits qui sont bien classifiés (vrais positifs) par rapport à tous les cas positifs prédits. C'est donc une mesure pour savoir à quel point on se trompe lorsqu'on prédit un cas positif.\n",
    "\n",
    "Pour les problèmes de classification ayant plus de deux étiquettes de classes, on peut utiliser l'approche de un-contre-tous. On calcule donc la précision de la classification binaire en utilisant une seule classe à la fois. La précision de la première classe serait donc lorsque cette classe a été prédit comme vrai et que c'était bien le cas par rapport à tous les cas positifs. \\\n",
    "Après avoir fait le calcul pour chaque classe, on peut faire la moyenne pondérée des précisions de chaque classe pour avoir la prédiction du modèle. La pondération se fait sur le nombre d'observation de chaque classe.\n",
    "\n",
    "#### Rappel\n",
    "Pour les problèmes de classification binaire, le rappel est le pourcentage des cas positif qui sont bien classifiés par rapport à tous les exemples qui sont réellement des cas positifs. C'est donc une mesure pour savoir à quel point on prédit bien les cas positifs.\n",
    "\n",
    "Pour les problèmes de classification ayant plus de deux étiquettes de classes, on utilise l'approche de un-contre-tous de la même façon que pour la précision, mais en utilisant le calcul du rappel.\n",
    "\n",
    "#### F1-score\n",
    "Le F1-score est la moyenne harmonique de la précision et du rappel. Cela permet d'avoir un équilibre entre ces deux mesures. Le F1-score a donc le même objectif que l'exactitude, mais il est moins sensible aux cas où la classe à prédire est rarement présente dans les instances.\n",
    "\n",
    "On utilise encore la même procédure pour les problèmes de classification ayant plus de deux étiquettes de classes.\n",
    "\n",
    "#### Matrice de confusion\n",
    "Pour les problèmes de classification binaire, la matrice de confusion contient deux lignes et deux colonnes. Elle sert à visualiser la performance d'un modèle. <br>\n",
    "Les lignes indiquent le nombre d'instance qui sont réellement dans la classe et le nombres d'instance qui ne sont pas dans la classe. <br>\n",
    "Les colonnes indiquent le nombre d'instance qui sont prédites dans la classe et le nombres d'instance qui ne sont pas prédits dans la classe. <br>\n",
    "On a donc le nombre de vrais positifs (VP) dans le coin supérieur gauche, le nombre de vrais négatifs (VN) dans le coin inférieur droit, le nombre de faux négatifs (FN) dans le coin supérieur droit et le nombre de faux positifs (FP) dans le coin inférieur gauche. <br>\n",
    "Une instance avec un vrai positif est lorsqu'on la prédit comme étant dans la classe alors que cette instance est réellement dans cette classe. <br>\n",
    "Une instance avec un vrai négatif est lorsqu'on la prédit comme n'étant pas dans la classe alors que cette instance n'est pas réellement dans cette classe. <br>\n",
    "Une instance avec un faux positif est lorsqu'on la prédit comme étant dans la classe alors que cette instance n'est pas réellement dans cette classe. <br>\n",
    "Une instance avec un vrai négatif est lorsqu'on la prédit comme n'étant pas dans la classe alors que cette instance est réellement dans cette classe.\n",
    "\n",
    "On peut calculer l’exactitude, la précision, le rappel et le F1-score à partir de la matrice de confusion. \n",
    "\n",
    "Pour calculer l'exactitude, on additionne le nombre de vrai positif et de faux positif et on divise la somme par le nombre total d'instance. Le nombre total d'instance est la somme de tout les éléments de la matrice.\n",
    "$$Exactitude = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Pour calculer la précision, on divise le nombre de vrai positif par la somme des éléments de la première colonne de la matrice. Cette première colonne est la somme du nombre de vrai positif et de faux positif.\n",
    "$$Précision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Pour calculer le rappel, on divise le nombre de vrai positif par la somme des éléments de la première ligne de la matrice. Cette première ligne est la somme du nombre de vrai positif et de faux négatif.\n",
    "$$Rappel = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Pour calculer le F1-score, on peut utiliser les résultats de la précision et du rappel, mais on peut aussi utiliser directement la matrice de confusion. Pour se faire, on utile la fonction suivant : \n",
    "$$F_1 = \\frac{TP}{TP + 0.5 * (FP + FN)}$$\n",
    "\n",
    "Pour les problèmes de classification ayant plus de deux étiquettes de classes, on peut utiliser l'approche un-contre-tous. Dans ce cas, on a autant de matrice de confusion qu'on a de classes, car on fait une matrice par classe. On essaie donc de prédire si l'instance est dans la classe ou pas pour chaque classe. Les métriques sont calculées sur chaque matrices et on peut faire les moyennes pondérées pour avoir les performances du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K plus proches voisins\n",
    "\n",
    "La métrique de distance utilisée est la distance euclidienne parce qu'on n'a pas un grand nombre d'attributs et ils sont tous numériques.  \n",
    "\n",
    "#### Pseudo-code\n",
    "**function** train(train_data, train_labels) <br>\n",
    "$\\quad$ Save train_data and train_labels in sample list\n",
    "\n",
    "**function** predict(x) <br>\n",
    "$\\quad$ Compute the Euclidean distance between x and all my points in my sample list <br>\n",
    "$\\quad$ Find the k nearest neighbors <br>\n",
    "$\\quad$ **return** the most popular label among the k nearest neighbors\n",
    "\n",
    "#### Comparaison de notre implémentation avec scikit-learn\n",
    "Nous obtenons quasiment les mêmes résultats pour les deux implémentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Naïve Bayésienne\n",
    "Nous avons décidé de faire la classifieur Naïf Bayésien Gaussien puisque la plupart des attributs sont numériques.\n",
    "#### Pseudo-code\n",
    "##### Fonction train\n",
    "**function** train(train_data, train_labels) <br>\n",
    "$\\quad$ prior = [ ]<br>\n",
    "$\\quad$ **For** each unique class $\\in$ train_labels <br>\n",
    "$\\quad\\quad$ #Calcul des probabilités a priori <br>\n",
    "$\\quad\\quad$ Add to prior $\\frac{number\\ of\\ instance\\ in\\ class}{total\\ number\\ of\\ instance}$\n",
    "\n",
    "$\\quad$ Initiate means, a matrix of number of class x number of features <br>\n",
    "$\\quad$ Initiate std, a matrix of number of class x number of features<br>\n",
    "$\\quad$ **For** each unique class $\\in$ train_labels <br>\n",
    "$\\quad\\quad$ cond = train_data for which the label is equal to class <br>\n",
    "$\\quad\\quad$ **For** each feature $\\in$ train_data <br>\n",
    "$\\quad\\quad\\quad$ means[class_position, feature_position] = mean of cond containing feature <br>\n",
    "$\\quad\\quad\\quad$ std[class_position, feature_position] = standard deviation of cond containing feature <br>\n",
    "$\\quad\\quad\\quad$ **End For** <br>\n",
    "$\\quad\\quad$ **End For**\n",
    "\n",
    "\n",
    "##### Fonction predict\n",
    "**function** predict(x) <br>\n",
    "$\\quad$ Initiate posterior_probs, a vector with a length of number of class <br>\n",
    "$\\quad$ **For** each unique class $\\in$ train_labels <br>\n",
    "$\\quad\\quad$ gaussian_probs = Gaussian density probabilities with x and the means and std associated with the class <br>\n",
    "$\\quad\\quad$ posterior_probs[class_position] = sum(log(gaussian_probs)) + log of the prior associated with the class <br>\n",
    "$\\quad\\quad$ **End For** <br>\n",
    "$\\quad$ **Return** the class with the maximum posterior_probs\n",
    "\n",
    "\n",
    "\n",
    "#### Comparaison de notre implémentation avec scikit-learn\n",
    "Pour les trois ensembles de données, on obtient exactement les mêmes résultats pour toutes les métriques avec notre implémentation qu'avec le classifieur de scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion et comparaison entre les deux techniques d'apprentissage\n",
    "\n",
    "\n",
    "<img src=\"IFT7025_TP3_comparison.PNG\" style=\"width:100%;\">\n",
    "\n",
    "On observe dans le tableau, les résultats des deux techniques d'apprentissage sur les données tests des trois jeux de données Iris, Wine et Abalone.\n",
    "\n",
    "Pour le jeu de donnée Iris, le KNN et le CNB on des performances plutôt similaire. Cependant, le CNB obtient des résultats légèrement inférieurs pour toutes les métriques.\n",
    "\n",
    "Pour le jeu de donnée Wine, la différence de performance entre les deux techniques est plus grande. Le KNN performe beaucoup mieux, surtout en terme de précision où il a une précision plus grande de plus de 14% par rapport au classifieur naïf Bayésien.\n",
    "\n",
    "Pour le jeu de donnée Abalone, l'écart de performance est encore plus grande que dans les autres jeux de données. L'exactitude, le rappel et le F1-score sont beaucoup plus élevé pour le KNN, cependant la précision est légèrement meilleure pour le CNB.\n",
    "\n",
    "La performance plus médiocre du classifieur naïf dans abalone est probablement expliquée par le fait que les attributs ne sont pas conditionnellement indépendants selon la classe. En effet, il serait surprenant que le poids des ormeaux soit conditionnellement indépendant de leurs dimensions. Il en est de même pour plusieurs autres attributs. Le KNN n'a pas la forte hypothèse d'indépendance conditionnelle donc ça ne nuit pas à ses performances.\n",
    "\n",
    "Dans les trois jeu données, le classifieur naif de Bayes est beaucoup plus rapide que les k plus proches voisins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "Pour le jeu de données iris, le classifieur naif de Bayes est préférable aux k plus proches voisins car le temps d'exécution de l'algorithme du CNB est largement inférieur à celui du KNN pour une différence minime dans les performances observées. Pour les jeux de données Wine et Abalone, il est préférable d'utiliser le KNN car cette technique performe beaucoup mieux que le classifieur naïf de Bayes. La différence de performance vaut probablement la différence dans le temps d'exécution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
